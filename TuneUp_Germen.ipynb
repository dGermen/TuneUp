{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rOfJ8lyD_f7l"
      },
      "source": [
        "### Pip Install Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_n6zk_Iy_izf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spektral\n",
            "  Downloading spektral-1.3.0-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m267.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (1.2.0)\n",
            "Collecting lxml (from spektral)\n",
            "  Using cached lxml-4.9.2-cp311-cp311-macosx_11_0_arm64.whl\n",
            "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (3.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (1.24.2)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (2.0.0)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (2.28.2)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (1.2.2)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from spektral) (4.65.0)\n",
            "Collecting tensorflow-macos>=2.5.0 (from spektral)\n",
            "  Downloading tensorflow_macos-2.12.0-cp311-cp311-macosx_12_0_arm64.whl (200.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astunparse>=1.6.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting flatbuffers>=2.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading h5py-3.8.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting jax>=0.3.15 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading jax-0.4.12.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading libclang-16.0.0-py2.py3-none-macosx_11_0_arm64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from spektral)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-macosx_11_0_arm64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from tensorflow-macos>=2.5.0->spektral) (23.1)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading protobuf-4.23.3-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from tensorflow-macos>=2.5.0->spektral) (67.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from tensorflow-macos>=2.5.0->spektral) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from tensorflow-macos>=2.5.0->spektral) (4.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.24.3 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading grpcio-1.54.2-cp311-cp311-macosx_10_10_universal2.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting tensorboard<2.13,>=2.12 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.13,>=2.12.0 (from tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from pandas->spektral) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from pandas->spektral) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->spektral) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->spektral) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->spektral) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->spektral) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos>=2.5.0->spektral) (0.40.0)\n",
            "Collecting ml-dtypes>=0.1.0 (from jax>=0.3.15->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral) (2.1.2)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos>=2.5.0->spektral)\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax, wrapt\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for jax: filename=jax-0.4.12-py3-none-any.whl size=1498447 sha256=91e983b8505adc123ebbe757e06411e9bff78292ee6b6a820c011a1f274df3b5\n",
            "  Stored in directory: /Users/dgermen/Library/Caches/pip/wheels/67/d8/8c/05507d30cc58cab62c405012c887ef3e6f4defa9b6d912a46d\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.14.1-cp311-cp311-macosx_11_0_arm64.whl size=21626 sha256=1c14da9b9616aa926d3e6fb34233ad16ada900a5cf546d878554479a9f020a59\n",
            "  Stored in directory: /Users/dgermen/Library/Caches/pip/wheels/eb/b6/fa/5ab6f4107cad63fa04c54ad78d75bb7035119bdd4f751df5ae\n",
            "Successfully built jax wrapt\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, markdown, lxml, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, ml-dtypes, h5py, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, spektral\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.2\n",
            "    Uninstalling numpy-1.24.2:\n",
            "      Successfully uninstalled numpy-1.24.2\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.20.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 jax-0.4.12 keras-2.12.0 libclang-16.0.0 lxml-4.9.2 markdown-3.4.3 ml-dtypes-0.2.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 spektral-1.3.0 tensorboard-2.12.3 tensorboard-data-server-0.7.1 tensorflow-estimator-2.12.0 tensorflow-macos-2.12.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.14.1\n",
            "Requirement already satisfied: ogb in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (2.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (1.26.15)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (67.6.1)\n",
            "Requirement already satisfied: littleutils in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (2.28.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: torch_geometric in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (2.3.0)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (2.28.2)\n",
            "Requirement already satisfied: pyparsing in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install spektral\n",
        "! pip install ogb\n",
        "! pip install torch_geometric"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_if-SzYqHGkc"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "\n",
        "from spektral.datasets.ogb import OGB\n",
        "from spektral.transforms import AdjToSpTensor, GCNFilter\n",
        "from ogb.nodeproppred import Evaluator, NodePropPredDataset\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "from itertools import product, combinations\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENUKDpqhHJWN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_edge_percentage = 0.2\n",
        "V_percentage = 0.05\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Related Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iV0jOcNYAVzV"
      },
      "source": [
        "## Dataset Related\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X4eb8naE_mYc"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OZPQxfI_oJ5",
        "outputId": "c5f2b348-6c59-40f5-f117-4640d232f890"
      },
      "outputs": [],
      "source": [
        "# import ogbn ogbn-arxiv\n",
        "\n",
        "\n",
        "dataset_name = \"ogbn-arxiv\"\n",
        "ogb_dataset = NodePropPredDataset(dataset_name)\n",
        "dataset = OGB(ogb_dataset, transforms=[GCNFilter(), AdjToSpTensor()])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_1LBrrALs6"
      },
      "source": [
        "### Converting dataset from TF to Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-CCwTpBEAAOQ"
      },
      "outputs": [],
      "source": [
        "# convert tf dataset to torch tensor\n",
        "\n",
        "\n",
        "\n",
        "# Get the node features, edge indices, and labels\n",
        "features = dataset[0].x\n",
        "edge_indices = dataset[0].a.indices\n",
        "labels = dataset[0].y\n",
        "\n",
        "# Convert TensorFlow tensors to PyTorch Tensors\n",
        "features_torch = torch.from_numpy(features)\n",
        "edge_indices_torch = torch.from_numpy(edge_indices.numpy().T).long()  # Transpose to fit PyG's edge_index format and convert to long\n",
        "labels_torch = torch.from_numpy(labels)\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(x=features_torch, edge_index=edge_indices_torch, y=labels_torch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D_0S2hqTjRYc"
      },
      "source": [
        "### Applying dataset splits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bu46Zf3yjQxh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # # # # #\n",
        "# Getting V and V_new\n",
        "# # # # # #\n",
        "\n",
        "# Assume that `data` is your PyTorch Geometric graph object.\n",
        "# data = ...\n",
        "\n",
        "# Get the number of nodes in your graph.\n",
        "num_nodes = data.num_nodes\n",
        "\n",
        "# Create a random permutation of indices [0, 1, 2, ..., num_nodes-1].\n",
        "perm = torch.randperm(num_nodes)\n",
        "\n",
        "# Calculate the index at which to split the permutation.\n",
        "split_idx = int(num_nodes * V_percentage)\n",
        "\n",
        "# Split the permutation into indices for V (95%) and V_new (5%).\n",
        "V = perm[:split_idx]\n",
        "V_new = perm[split_idx:]\n",
        "\n",
        "# V and V_new are now the indices of the nodes in the 95% and 5% splits, respectively.\n",
        "\n",
        "# ------> For node classification\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wuhb_V5nqeMh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # # # # #\n",
        "# Splitting edges to training and validation edges\n",
        "# # # # # #\n",
        "\n",
        "\n",
        "\n",
        "# Assuming your data is in this format\n",
        "# data = Data(x=features_torch, edge_index=edge_indices_torch, y=labels_torch)\n",
        "\n",
        "# Get the number of edges\n",
        "num_edges = data.edge_index.size(1)\n",
        "\n",
        "# Create a list of indices representing the edges\n",
        "edge_indices = list(range(num_edges))\n",
        "\n",
        "# Shuffle the indices randomly\n",
        "random.shuffle(edge_indices)\n",
        "\n",
        "# Define the percentage of edges to be used for training\n",
        "num_train_edges = int(train_edge_percentage * num_edges)\n",
        "\n",
        "# Split the indices into two sets: for training and validation\n",
        "train_edge_indices = edge_indices[:num_train_edges]\n",
        "val_edge_indices = edge_indices[num_train_edges:]\n",
        "\n",
        "# Function to create a new edge_index tensor based on selected indices\n",
        "def create_edge_index_subset(edge_index, selected_indices):\n",
        "    return edge_index[:, selected_indices]\n",
        "\n",
        "# Create new edge_index tensors for training and validation\n",
        "E_train = create_edge_index_subset(data.edge_index, train_edge_indices)\n",
        "E_val = create_edge_index_subset(data.edge_index, val_edge_indices)\n",
        "\n",
        "# Now, 'edge_index_train' contains the edges for the training set,\n",
        "# and 'edge_index_val' contains the edges for the validation set.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MmxBJg8YAipq"
      },
      "source": [
        "## Model Related\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0hIktgQmAr46"
      },
      "source": [
        "### Classical Backbone Model (GCN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NgPkgQAqAlq7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define a simple GNN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 256)\n",
        "        self.conv2 = GCNConv(256, 256)\n",
        "        self.conv3 = GCNConv(256, 256)\n",
        "\n",
        "        self.scoring = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * 256, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data, edge_index):\n",
        "        x = self.conv1(data.x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def decode(self, z, indices):\n",
        "        start, end = indices\n",
        "        edge_features = torch.cat([z[start], z[end]], dim=1)\n",
        "        return self.scoring(edge_features).squeeze(-1)\n",
        "\n",
        "\n",
        "def bpr_loss(pos_logit, neg_logit):\n",
        "    return -F.logsigmoid(pos_logit - neg_logit).sum()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "coB84tUNBAT0"
      },
      "source": [
        "## Training, Validation Test\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojvVNwRbBRmi"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0Kcc63SxBRAF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(model,V, data, train_edges, val_edges, optimizer, patience=10, epochs = 1000, test_active = True):\n",
        "\n",
        "  # Define some initial best validation loss as infinity\n",
        "  best_val_loss = float('inf')\n",
        "  epochs_no_improve = 0\n",
        "\n",
        "  # Training loop\n",
        "  data, train_edges, val_edges = data.to(device), train_edges.to(device), val_edges.to(device)\n",
        "  for epoch in range(epochs):  # 1000 epochs\n",
        "      print(\"epoch \", epoch)\n",
        "\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      z_train = model(data, train_edges)  # embeddings for training edges\n",
        "      pos_edge_index = train_edges  # positive examples\n",
        "      neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z_train.size(0))  # negative examples\n",
        "\n",
        "      #print(\"pos_edge_index.shape: \", pos_edge_index.shape)\n",
        "      pos_logit = model.decode(z_train, pos_edge_index)\n",
        "      neg_logit = model.decode(z_train, neg_edge_index)\n",
        "\n",
        "      loss = bpr_loss(pos_logit, neg_logit)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print(\"train loss: \", loss.item())\n",
        "\n",
        "      if test_active:\n",
        "        res = test(model, V, val_edges, 50)\n",
        "        print(\"recall@50: \", res)\n",
        "\n",
        "      # Validation:\n",
        "      if (epoch +1) % 5 == 0:\n",
        "        # validation function calls model.eval(), calculating both val loss & recall@50\n",
        "        val_loss, recall_50 = validation(model, data, val_edges, 50)\n",
        "        print(f'Validation Loss: {val_loss}, Recall@50: {recall_50}')\n",
        "\n",
        "        # Check if early stopping conditions are met\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs.')\n",
        "                break\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BIxQmBXmBY7x"
      },
      "source": [
        "### Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cXaxPHRGBjgF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def validation(model, nodes, val_edges, z, k=50):\n",
        "    #model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "\n",
        "        # Convert V to a boolean tensor for faster lookup.\n",
        "        v_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        v_mask[nodes] = True\n",
        "\n",
        "        # Assume val_edges contains the validation edges (it should be a 2 x num_val_edges tensor)\n",
        "        # val_edges = ...\n",
        "\n",
        "        # Check if both nodes of each edge in val_edges are in V\n",
        "        source_nodes = val_edges[0, :]\n",
        "        target_nodes = val_edges[1, :]\n",
        "        can_exist_in_V = v_mask[source_nodes] & v_mask[target_nodes]\n",
        "\n",
        "        # Filter the edges that can exist in V\n",
        "        valid_edges_in_V = val_edges[:, can_exist_in_V]\n",
        "        positive_pairs = valid_edges_in_V\n",
        "\n",
        "        # --- Generating negative pairs ---\n",
        "\n",
        "        # Find the unique starting nodes in val_edges\n",
        "        start_nodes = torch.unique(val_edges[0, :])\n",
        "\n",
        "        # Generate all possible pairs from start_nodes to all nodes in V\n",
        "        all_possible_pairs = torch.tensor(list(product(start_nodes.tolist(), V.tolist())))\n",
        "\n",
        "        # Remove the existing edges in val_edges from all_possible_pairs to create the negative pairs\n",
        "        existing_pairs = valid_edges_in_V.t()\n",
        "        negative_pairs = []\n",
        "        for pair in all_possible_pairs:\n",
        "            if not any(torch.all(pair == existing_pair, dim=0) for existing_pair in existing_pairs):\n",
        "                negative_pairs.append(pair)\n",
        "\n",
        "        negative_pairs = torch.stack(negative_pairs).t()\n",
        "\n",
        "\n",
        "          # Negative examples for validation\n",
        "\n",
        "        pos_logit_val = model.decode(z, positive_pairs)\n",
        "        neg_logit_val = model.decode(z, negative_pairs)\n",
        "\n",
        "        val_loss = bpr_loss(pos_logit_val, neg_logit_val)\n",
        "\n",
        "    return val_loss.item()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aItXSzmQBehU"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oosXH877XuJg"
      },
      "outputs": [],
      "source": [
        "def test(model, nodes, val_edges, z, k=50):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Take 5 samples from val_edges as positive examples\n",
        "    val_edges = val_edges[:, torch.randint(val_edges.size(1), (5,))]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Convert V to a boolean tensor for faster lookup.\n",
        "        v_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        v_mask[nodes] = True\n",
        "        v_mask = v_mask.to(device)\n",
        "\n",
        "        # Assume val_edges contains the validation edges (it should be a 2 x num_val_edges tensor)\n",
        "        # val_edges = ...\n",
        "\n",
        "        # Check if both nodes of each edge in val_edges are in V\n",
        "        source_nodes = val_edges[0, :]\n",
        "        target_nodes = val_edges[1, :]\n",
        "        can_exist_in_V = v_mask[source_nodes] & v_mask[target_nodes]\n",
        "\n",
        "        # Filter the edges that can exist in V\n",
        "        valid_edges_in_V = val_edges[:, can_exist_in_V]\n",
        "        positive_pairs = valid_edges_in_V\n",
        "\n",
        "        # --- Generating negative pairs ---\n",
        "\n",
        "        # Find the unique starting nodes in val_edges\n",
        "        start_nodes = torch.unique(val_edges[0, :])\n",
        "\n",
        "        # Generate all possible pairs from start_nodes to all nodes in V\n",
        "        all_possible_pairs = torch.tensor(list(product(start_nodes.tolist(), V.tolist())))\n",
        "\n",
        "        # Remove the existing edges in val_edges from all_possible_pairs to create the negative pairs\n",
        "        existing_pairs = valid_edges_in_V.t()\n",
        "        negative_pairs = []\n",
        "        negative_pairs.to(device)\n",
        "        \n",
        "        for pair in all_possible_pairs:\n",
        "            if not any(torch.all(pair == existing_pair, dim=0) for existing_pair in existing_pairs):\n",
        "                negative_pairs.append(pair)\n",
        "\n",
        "        negative_pairs = torch.stack(negative_pairs).t()\n",
        "\n",
        "\n",
        "          # Negative examples for validation\n",
        "\n",
        "        positive_scores = model.decode(z, positive_pairs)\n",
        "        negative_scores = model.decode(z, negative_pairs)\n",
        "\n",
        "        # Combine positive and negative scores\n",
        "        all_scores = torch.cat([positive_scores, negative_scores])\n",
        "\n",
        "        # Indicate which edges are positive (1 for positive, 0 for negative)\n",
        "        positive_edge_indicator = torch.tensor([1]*val_edges.size(1) + [0]*negative_pairs.size(1))\n",
        "\n",
        "\n",
        "        recall_per_node = calculate_recall_per_node(start_nodes, all_scores, positive_edge_indicator, k)\n",
        "\n",
        "        # Calculate the average recall over all starting nodes\n",
        "        recall = recall_per_node.mean()\n",
        "\n",
        "        return recall.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Etz_l6DdBNNT"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cYraB198HlZN"
      },
      "source": [
        "### Recall calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjkZ9NXuBMPf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# recall_at_k_per_node(model, z, val_edges, k, unique_nodes, data.edge_index)\n",
        "def recall_at_k():\n",
        "\n",
        "  # Count all relevant target nodes for the nodes\n",
        "  total_rel = None\n",
        "\n",
        "  # Count number of relevant edges at k\n",
        "  total_rel_k = None\n",
        "\n",
        "  # Ratio them and average\n",
        "\n",
        "  recall = total_rel_k / total_rel\n",
        "  recall_avg = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # get val nodes\n",
        "  val_nodes = torch.unique(val_edges)\n",
        "\n",
        "  # get all the real edges from the val nodes: pos_v\n",
        "  mask = torch.isin(edge_index, val_nodes).any(dim=0)\n",
        "  positive_edges = edge_index[:, mask]\n",
        "\n",
        "  # get all possible edges: all_v\n",
        "  all_edges_val = list(itertools.product(val_nodes.tolist(), unique_nodes.tolist()))\n",
        "  all_edges_val = torch.tensor(all_edges_val, dtype=torch.long).t().contiguous()\n",
        "\n",
        "  # get scores for all possible edges\n",
        "  scores = model.decode(z, all_edges_val)\n",
        "\n",
        "  # Get top k scores and their corresponding edges\n",
        "  _, top_k_indices = scores.topk(k, largest=True)\n",
        "  top_k_edges = all_edges_val[:, top_k_indices.cpu()]\n",
        "\n",
        "  # check how many of them in top50\n",
        "  top_k_edges = set( tuple( sorted((int(n1), int(n2))) ) for n1, n2 in zip(top_k_edges[0], top_k_edges[1]))\n",
        "  positive_edges = set(tuple(sorted((int(n1), int(n2)))) for n1, n2 in zip(positive_edges[0], positive_edges[1]))\n",
        "\n",
        "  print(\"top_k_edges: \\n\", top_k_edges)\n",
        "  print(\"positive_edges \\n\", positive_edges)\n",
        "\n",
        "  # calculate recall@k\n",
        "  num_hits = len(top_k_edges & graph2_edges)\n",
        "  recall_at_K = num_hits / len(positive_edges)\n",
        "\n",
        "  return recall_at_K\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OrBM-tpoBkC9"
      },
      "outputs": [],
      "source": [
        "def calculate_recall_per_node(start_nodes, all_scores, positive_edge_indicator, K):\n",
        "    \"\"\"\n",
        "    Calculate recall for each individual starting node using tensor operations.\n",
        "    \n",
        "    Parameters:\n",
        "    - start_nodes: Tensor of shape [num_edges], containing the starting node of each edge.\n",
        "    - all_scores: Tensor of shape [num_edges], containing scores for each edge.\n",
        "    - positive_edge_indicator: Tensor of shape [num_edges], containing 1 for positive edges and 0 for negative edges.\n",
        "    - K: The number of top edges to consider for calculating recall.\n",
        "    \n",
        "    Returns:\n",
        "    - recall_per_node: Dictionary with nodes as keys and recall as values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort scores in descending order\n",
        "    sorted_indices = torch.argsort(all_scores, descending=True)\n",
        "\n",
        "    # Apply sorting to start nodes and positive indicators\n",
        "    sorted_start_nodes = start_nodes[sorted_indices]\n",
        "    sorted_positive_indicators = positive_edge_indicator[sorted_indices]\n",
        "\n",
        "    # Count the number of positive edges in top K for each unique start node\n",
        "    unique_start_nodes, counts = torch.unique(sorted_start_nodes, return_counts=True)\n",
        "    top_k_mask = (counts > K).int() * K + (counts <= K).int() * counts\n",
        "    accumulated_positive = sorted_positive_indicators.cumsum(dim=0)\n",
        "    total_positive_in_top_k = accumulated_positive[top_k_mask - 1]\n",
        "\n",
        "    # Calculate recall for each unique start node\n",
        "    recalls = total_positive_in_top_k.float() / K\n",
        "\n",
        "    # Create a dictionary mapping start node to recall\n",
        "    recall_per_node = {node.item(): recall.item() for node, recall in zip(unique_start_nodes, recalls)}\n",
        "\n",
        "    return recall_per_node"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TuneUP: Synthesizing tail nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.utils import degree\n",
        "\n",
        "def renormalize(edge_index, num_nodes):\n",
        "    # Convert to PyTorch tensor for calculation\n",
        "    edge_index = edge_index.clone().detach()\n",
        "    \n",
        "    # Calculate degree and create Degree Matrix D\n",
        "    row, col = edge_index\n",
        "    deg = degree(row, num_nodes, dtype=edge_index.dtype)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
        "\n",
        "    # Renormalize\n",
        "    row, col = edge_index\n",
        "    edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "    return edge_index, edge_weight\n",
        "\n",
        "\n",
        "def random_edge_sampler(data, percent):\n",
        "    edge_index = data.edge_index\n",
        "    num_nodes = data.num_nodes\n",
        "\n",
        "    num_edges = edge_index.size(1)\n",
        "    perm = torch.randperm(num_edges)\n",
        "    preserve_nnz = int(num_edges * percent)\n",
        "\n",
        "    # Indices for kept edges\n",
        "    kept_indices = perm[:preserve_nnz]\n",
        "    kept_edges = edge_index[:, kept_indices]\n",
        "    kept_edges, kept_weights = renormalize(kept_edges, num_nodes)\n",
        "    data_kept = Data(edge_index=kept_edges, edge_attr=kept_weights)\n",
        "\n",
        "    # Indices for dropped edges\n",
        "    dropped_indices = perm[preserve_nnz:]\n",
        "    dropped_edges = edge_index[:, dropped_indices]\n",
        "    dropped_edges, dropped_weights = renormalize(dropped_edges, num_nodes)\n",
        "    data_dropped = Data(edge_index=dropped_edges, edge_attr=dropped_weights)\n",
        "\n",
        "    return data_kept, data_dropped\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## USAGE:\n",
        "\n",
        "# percent: rate of edges to keep\n",
        "data_kept, data_dropped = random_edge_sampler(data, percent)\n",
        "\n",
        "## INPUT:\n",
        "Data(x=[169343, 128], edge_index=[2, 1335586], y=[169343, 1])\n",
        "\n",
        "## RETURNS:\n",
        "(Data(edge_index=[2, 468243]), Data(edge_index=[2, 962188]))\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Execution \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  0\n"
          ]
        }
      ],
      "source": [
        "model = GCN(128)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)  # L2 regularization\n",
        "\n",
        "train(model, V, data, train_edges=E_train, val_edges=E_val, optimizer=optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = {\"a\": 1, \"b\": 2}\n",
        "\n",
        "a = A.values()\n",
        "a = list(a)\n",
        "\n",
        "# turn a to np array\n",
        "a = np.array(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([169343])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "v_mask.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "169343"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[169343, 128], edge_index=[2, 1335586], y=[169343, 1])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intersection:\n",
            "tensor([[ 2,  3, 11],\n",
            "        [ 6,  7, 13]], dtype=torch.int32)\n",
            "B without intersection:\n",
            "tensor([[ 9, 15],\n",
            "        [10, 16]], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define tensors\n",
        "E_all = torch.tensor([\n",
        "    [1, 2, 3, 4, 11, 12],\n",
        "    [5, 6, 7, 8, 13, 14]\n",
        "], dtype=torch.int)\n",
        "\n",
        "B = torch.tensor([\n",
        "    [2, 3, 9, 11, 15],\n",
        "    [6, 7, 10, 13, 16]\n",
        "], dtype=torch.int)\n",
        "\n",
        "# Check if a GPU is available and if so, use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "E_all = E_all.to(device)\n",
        "B = B.to(device)\n",
        "\n",
        "# Compute the pairwise equality\n",
        "pairwise_equality = torch.eq(E_all.unsqueeze(2), B.unsqueeze(1))\n",
        "\n",
        "# Determine the columns where all rows are True (i.e., both elements in column are equal)\n",
        "column_equality = torch.all(pairwise_equality, dim=0)\n",
        "\n",
        "# Extract the intersecting columns\n",
        "intersection = B[:, column_equality.any(dim=0)]\n",
        "\n",
        "# Remove intersection from B\n",
        "B_without_intersection = B[:, ~column_equality.any(dim=0)]\n",
        "\n",
        "# Display the intersection and B without intersection\n",
        "print(\"Intersection:\")\n",
        "print(intersection.cpu())\n",
        "print(\"B without intersection:\")\n",
        "print(B_without_intersection.cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
            "        [5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8]], dtype=torch.int32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/TUI/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assume start_nodes and V are one-dimensional tensors representing sets of nodes\n",
        "start_nodes = torch.tensor([1, 2, 3, 4], dtype=torch.int)\n",
        "V = torch.tensor([5, 6, 7, 8], dtype=torch.int)\n",
        "\n",
        "# Check if a GPU is available and if so, use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "start_nodes = start_nodes.to(device)\n",
        "V = V.to(device)\n",
        "\n",
        "# Create all possible pairs tensor using broadcasting\n",
        "all_possible_pairs = torch.stack(torch.meshgrid(start_nodes, V), dim=-1).reshape(-1, 2).t()\n",
        "\n",
        "# Display all possible pairs\n",
        "print(all_possible_pairs.cpu())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X4eb8naE_mYc"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
