{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rOfJ8lyD_f7l"
      },
      "source": [
        "### Pip Install Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n6zk_Iy_izf",
        "outputId": "d9bfbf86-8703-43bf-d0f9-2ce817cba666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spektral in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from spektral) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from spektral) (4.9.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from spektral) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spektral) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from spektral) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from spektral) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spektral) (4.65.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from spektral) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.32.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.2.0->spektral) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.15)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pdb (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pdb\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install spektral\n",
        "! pip install ogb\n",
        "! pip install torch_geometric\n",
        "! pip install pdb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_if-SzYqHGkc"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "AFd0sWJArntD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "from spektral.datasets.ogb import OGB\n",
        "from spektral.transforms import AdjToSpTensor, GCNFilter\n",
        "from ogb.nodeproppred import Evaluator, NodePropPredDataset\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "from itertools import product, combinations\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "import itertools\n",
        "\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENUKDpqhHJWN",
        "outputId": "7a3b4616-bd1f-4b74-e123-766ceb07d3fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc700302c10>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device_name=       'mps'\n",
        "elif torch.cuda.is_available():\n",
        "    device_name= 'cuda'\n",
        "else:\n",
        "    device_name=  'cpu'\n",
        "device = torch.device(device_name)\n",
        "\n",
        "\n",
        "train_edge_percentage = 0.5\n",
        "V_percentage = 0.95\n",
        "SEED = 42\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Seed everything for deterministic runs\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Seed torch\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yQLn0tkcrntE"
      },
      "source": [
        "### Training Related Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mke-cw8prntE",
        "outputId": "4b12c004-92b9-4fc7-964b-1233721dfd66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iV0jOcNYAVzV"
      },
      "source": [
        "## Dataset Related\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X4eb8naE_mYc"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "2OZPQxfI_oJ5"
      },
      "outputs": [],
      "source": [
        "# import ogbn ogbn-arxiv\n",
        "\n",
        "\n",
        "dataset_name = \"ogbn-arxiv\"\n",
        "ogb_dataset = NodePropPredDataset(dataset_name)\n",
        "dataset = OGB(ogb_dataset, transforms=[GCNFilter(), AdjToSpTensor()])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_1LBrrALs6"
      },
      "source": [
        "### Converting dataset from TF to Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-CCwTpBEAAOQ"
      },
      "outputs": [],
      "source": [
        "# convert tf dataset to torch tensor\n",
        "\n",
        "\n",
        "\n",
        "# Get the node features, edge indices, and labels\n",
        "features = dataset[0].x\n",
        "edge_indices = dataset[0].a.indices\n",
        "labels = dataset[0].y\n",
        "\n",
        "# Convert TensorFlow tensors to PyTorch Tensors\n",
        "features_torch = torch.from_numpy(features)\n",
        "edge_indices_torch = torch.from_numpy(edge_indices.numpy().T).long()  # Transpose to fit PyG's edge_index format and convert to long\n",
        "labels_torch = torch.from_numpy(labels)\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(x=features_torch, edge_index=edge_indices_torch, y=labels_torch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D_0S2hqTjRYc"
      },
      "source": [
        "### Applying dataset splits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Bu46Zf3yjQxh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # # # # #\n",
        "# Getting V and V_new\n",
        "# # # # # #\n",
        "\n",
        "# Assume that `data` is your PyTorch Geometric graph object.\n",
        "# data = ...\n",
        "\n",
        "# Get the number of nodes in your graph.\n",
        "num_nodes = data.num_nodes\n",
        "\n",
        "# Create a random permutation of indices [0, 1, 2, ..., num_nodes-1].\n",
        "perm = torch.randperm(num_nodes)\n",
        "\n",
        "# Calculate the index at which to split the permutation.\n",
        "split_idx = int(num_nodes * V_percentage)\n",
        "\n",
        "# Split the permutation into indices for V (95%) and V_new (5%).\n",
        "V = perm[:split_idx].to(device)\n",
        "V_new = perm[split_idx:].to(device)\n",
        "\n",
        "# V and V_new are now the indices of the nodes in the 95% and 5% splits, respectively.\n",
        "\n",
        "# ------> For node classification\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Wuhb_V5nqeMh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # # # # #\n",
        "# Splitting edges to training and validation edges\n",
        "# # # # # #\n",
        "\n",
        "\n",
        "\n",
        "# Assuming your data is in this format\n",
        "# data = Data(x=features_torch, edge_index=edge_indices_torch, y=labels_torch)\n",
        "\n",
        "# Get the number of edges\n",
        "num_edges = data.edge_index.size(1)\n",
        "\n",
        "# Create a list of indices representing the edges\n",
        "edge_indices = list(range(num_edges))\n",
        "\n",
        "# Shuffle the indices randomly\n",
        "random.shuffle(edge_indices)\n",
        "\n",
        "# Define the percentage of edges to be used for training\n",
        "num_train_edges = int(train_edge_percentage * num_edges)\n",
        "\n",
        "# Split the indices into two sets: for training and validation\n",
        "train_edge_indices = edge_indices[:num_train_edges]\n",
        "val_edge_indices = edge_indices[num_train_edges:]\n",
        "\n",
        "# Function to create a new edge_index tensor based on selected indices\n",
        "def create_edge_index_subset(edge_index, selected_indices):\n",
        "    return edge_index[:, selected_indices]\n",
        "\n",
        "# Create new edge_index tensors for training and validation\n",
        "E_train = create_edge_index_subset(data.edge_index, train_edge_indices).to(device)\n",
        "E_val = create_edge_index_subset(data.edge_index, val_edge_indices).to(device)\n",
        "E_all = data.edge_index.to(device).to(device)\n",
        "\n",
        "# Now, 'edge_index_train' contains the edges for the training set,\n",
        "# and 'edge_index_val' contains the edges for the validation set.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6kISsHbVmlmx"
      },
      "source": [
        "### Pre sepearting according to starting nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "uuAAPFkemlmx"
      },
      "outputs": [],
      "source": [
        "# We want to create a dict that gives positive edges for a given node (key)\n",
        "\n",
        "# Can it exists in V?\n",
        "\n",
        "v_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "v_mask[V] = True\n",
        "v_mask = v_mask.to(device)\n",
        "\n",
        "source_nodes = E_val[0, :]\n",
        "target_nodes = E_val[1, :]\n",
        "can_exist_in_V = v_mask[source_nodes] & v_mask[target_nodes]\n",
        "valid_edges_in_V = E_val[:, can_exist_in_V]\n",
        "\n",
        "start_nodes = torch.unique(valid_edges_in_V)\n",
        "\n",
        "start_node_dict_val = {}\n",
        "for start_node in start_nodes:\n",
        "    start_node = int(start_node)\n",
        "    mask = valid_edges_in_V[0] == start_node\n",
        "    edges = valid_edges_in_V[:, mask]\n",
        "    start_node_dict_val[start_node] = edges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q66UL0B3mlmx",
        "outputId": "f09e8fb5-ba96-45dc-bac5-03b88dc49311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0, device='cuda:0')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_nodes[0]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MmxBJg8YAipq"
      },
      "source": [
        "## Model Related\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0hIktgQmAr46"
      },
      "source": [
        "### Classical Backbone Model (GCN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NgPkgQAqAlq7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define a simple GNN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 256)\n",
        "        self.conv2 = GCNConv(256, 256)\n",
        "        self.conv3 = GCNConv(256, 256)\n",
        "\n",
        "        self.scoring = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * 256, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data, edge_index):\n",
        "        x = self.conv1(data.x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def decode(self, z, indices):\n",
        "        start, end = indices\n",
        "        edge_features = torch.cat([z[start], z[end]], dim=1)\n",
        "        return self.scoring(edge_features).squeeze(-1)\n",
        "\n",
        "\n",
        "def bpr_loss(pos_logit, neg_logit):\n",
        "    #return F.log(F.sigmoid(pos_logit - neg_logit)).sum()\n",
        "    return -F.logsigmoid(pos_logit - neg_logit).sum()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "coB84tUNBAT0"
      },
      "source": [
        "## Training, Validation Test\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojvVNwRbBRmi"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "0Kcc63SxBRAF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(model,V, data, train_edges, val_edges, optimizer, patience=10, epochs = 1000, test_active = True):\n",
        "\n",
        "  # Define some initial best validation loss as infinity\n",
        "  best_val_loss = float('inf')\n",
        "  epochs_no_improve = 0\n",
        "\n",
        "  # Training loop\n",
        "  data, train_edges, val_edges = data.to(device), train_edges.to(device), val_edges.to(device)\n",
        "  for epoch in range(epochs):  # 1000 epochs\n",
        "      print(\"epoch \", epoch)\n",
        "\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      z_train = model(data, train_edges)  # embeddings for training edges\n",
        "      pos_edge_index = train_edges  # positive examples\n",
        "      neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z_train.size(0))  # negative examples\n",
        "\n",
        "      #print(\"pos_edge_index.shape: \", pos_edge_index.shape)\n",
        "      pos_logit = model.decode(z_train, pos_edge_index)\n",
        "      neg_logit = model.decode(z_train, neg_edge_index)\n",
        "\n",
        "      loss = bpr_loss(pos_logit, neg_logit)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print(\"train loss: \", loss.item())\n",
        "\n",
        "\n",
        "\n",
        "      # Validation:\n",
        "      if (epoch +1) % 5 == 0:\n",
        "        # validation function calls model.eval(), calculating both val loss & recall@50\n",
        "        val_loss = validation(model, V, val_edges, z_train)\n",
        "        print(f'Validation Loss: {val_loss}')\n",
        "        if test_active:\n",
        "          res = test(model, V, val_edges,z_train, 50)\n",
        "          print(\"recall@50: \", res)\n",
        "        # Check if early stopping conditions are met\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs.')\n",
        "                break\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BIxQmBXmBY7x"
      },
      "source": [
        "### Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cXaxPHRGBjgF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def validation(model, nodes, val_edges, z):\n",
        "    #model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "      pos_edge_index = val_edges  # positive examples\n",
        "      neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=z.size(0))  # negative examples\n",
        "\n",
        "\n",
        "          # Negative examples for validation\n",
        "\n",
        "      pos_logit = model.decode(z, pos_edge_index)\n",
        "      neg_logit = model.decode(z, neg_edge_index)\n",
        "\n",
        "      val_loss = bpr_loss(pos_logit, neg_logit)\n",
        "\n",
        "    return val_loss.item()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aItXSzmQBehU"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "oosXH877XuJg"
      },
      "outputs": [],
      "source": [
        "def test(model, nodes, val_edges, z, k=50):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # add this line to set the device\n",
        "\n",
        "    # Take 5 samples from val_edges as positive examples\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Convert V to a boolean tensor for faster lookup.\n",
        "        v_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        v_mask[nodes] = True\n",
        "        v_mask = v_mask.to(device)\n",
        "\n",
        "        # Assume val_edges contains the validation edges (it should be a 2 x num_val_edges tensor)\n",
        "        # val_edges = ...\n",
        "\n",
        "        # Check if both nodes of each edge in val_edges are in V\n",
        "        source_nodes = val_edges[0, :]\n",
        "        target_nodes = val_edges[1, :]\n",
        "        can_exist_in_V = v_mask[source_nodes] & v_mask[target_nodes]\n",
        "\n",
        "        can_exist_in_V.to(device)\n",
        "\n",
        "        # Filter the edges that can exist in V\n",
        "        valid_edges_in_V = val_edges[:, can_exist_in_V].to(device)\n",
        "        positive_pairs = valid_edges_in_V\n",
        "\n",
        "\n",
        "        # FOR MEMORY\n",
        "        selected_pairs = positive_pairs[:, torch.randint(valid_edges_in_V.size(1), (500,))]\n",
        "\n",
        "\n",
        "        # --- Generating negative pairs ---\n",
        "\n",
        "        # Find the unique starting nodes in val_edges\n",
        "        start_nodes = torch.unique(selected_pairs[0, :]).to(device)\n",
        "\n",
        "        # Tour over start nodes\n",
        "        a = time.time()\n",
        "        for start_node in start_nodes:\n",
        "            timezzz = time.time()\n",
        "\n",
        "\n",
        "            all_possible_pairs = torch.stack(torch.meshgrid(start_node, V), dim=-1).reshape(-1, 2).t().to(device)\n",
        "\n",
        "            # Clock time for look up and print it\n",
        "            start_node = int(start_node)\n",
        "            positive_pairs = start_node_dict_val[start_node] # THIS IS HARDCODED GLOBAL VARIABLE DO NOT COPY PASTE\n",
        "\n",
        "\n",
        "            # Remove the existing edges in val_edges from all_possible_pairs to create the negative pairs\n",
        "            existing_pairs = positive_pairs.t()\n",
        "            existing_pairs = existing_pairs.to(device)\n",
        "\n",
        "            # Removing positive pairs that are generated accidentaly\n",
        "            negative_pairs = remove_common_edges(E_all=positive_pairs,B=all_possible_pairs) # B - (A INTERSECTION B)\n",
        "\n",
        "            # Negative examples for validation\n",
        "            timezzzz = time.time()\n",
        "            positive_scores = model.decode(z, positive_pairs)\n",
        "            negative_scores = model.decode(z, negative_pairs)\n",
        "\n",
        "\n",
        "            # Combine positive edges and negative scores\n",
        "            all_edges = torch.cat([positive_pairs, negative_pairs], dim=1)\n",
        "            all_scores = torch.cat([positive_scores, negative_scores])\n",
        "            # Indicate which edges are positive (1 for positive, 0 for negative)\n",
        "            positive_edge_indicator = torch.tensor([1]*positive_pairs.size(1) + [0]*negative_pairs.size(1)).to(device)\n",
        "\n",
        "\n",
        "            recall= calculate_recall_per_node(all_edges, all_scores, positive_edge_indicator, k,start_node)\n",
        "            # Report passed time\n",
        "\n",
        "\n",
        "            del all_possible_pairs\n",
        "            del all_scores\n",
        "            del all_edges\n",
        "        print(\"time\")\n",
        "        print(a - time.time())\n",
        "        return recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "D2Uiu70orntH"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Etz_l6DdBNNT"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cYraB198HlZN"
      },
      "source": [
        "### Recall calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "OrBM-tpoBkC9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def calculate_recall_per_node(all_edges, all_scores, positive_edge_indicator, K, start_node):\n",
        "    \"\"\"\n",
        "    Calculate recall for each individual starting node using tensor operations on GPU.\n",
        "\n",
        "    Parameters:\n",
        "    - all_edges: Tensor of shape [2, num_edges], containing edges (source -> target).\n",
        "    - all_scores: Tensor of shape [num_edges], containing scores for each edge.\n",
        "    - positive_edge_indicator: Tensor of shape [num_edges], containing 1 for positive edges and 0 for negative edges.\n",
        "    - K: The number of top edges to consider for calculating recall.\n",
        "\n",
        "    Returns:\n",
        "    - recall_per_node: Dictionary with nodes as keys and recall as values.\n",
        "    \"\"\"\n",
        "\n",
        "    #print(\"all_edges\")\n",
        "    #print(all_edges.shape)\n",
        "\n",
        "    #print(\"all_scores\")\n",
        "    #print(all_scores.shape)\n",
        "\n",
        "    #print(\"positive_edge_indicator\")\n",
        "    #print(positive_edge_indicator.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Sort scores in descending order\n",
        "    sorted_indices = torch.argsort(all_scores, descending=True)\n",
        "\n",
        "    positive_edge_indicator = positive_edge_indicator[sorted_indices]\n",
        "\n",
        "    recall =  positive_edge_indicator[:K].sum() / positive_edge_indicator.sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create bins for each unique start node\n",
        "    #bins = torch.zeros_like(all_scores).scatter_(0, all_edges[0, :], 1).cumsum(0)\n",
        "\n",
        "    # Create a mask for top K elements in each bin\n",
        "    #top_k_mask = (bins <= K).gather(0, torch.argsort(bins.gather(0, sorted_indices)))\n",
        "\n",
        "    # Compute recalls by start node\n",
        "    #top_k_sorted_positive_indicators = positive_edge_indicator[sorted_indices][top_k_mask]\n",
        "    #recall = (top_k_sorted_positive_indicators.view(len(start_nodes), -1).sum(1) / K).cpu().numpy()\n",
        "\n",
        "    # Create recall_per_node dictionary\n",
        "    #recall_per_node = {node.item(): recall for node, recall in zip(start_nodes, recalls)}\n",
        "\n",
        "    return recall\n",
        "\n",
        "# Example usage remains the same\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ME7OGQAquvy7"
      },
      "source": [
        "### Find intersection, remove it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "3C-qC2l-uz4A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def remove_common_edges(E_all, B):\n",
        "    return B\n",
        "    # Compute the pairwise equality\n",
        "    pairwise_equality = torch.eq(E_all.unsqueeze(2), B.unsqueeze(1))\n",
        "\n",
        "    # Determine the columns where all rows are True (i.e., both elements in column are equal)\n",
        "    column_equality = torch.all(pairwise_equality, dim=0)\n",
        "\n",
        "    # Clear intermediate tensor\n",
        "    del pairwise_equality\n",
        "\n",
        "    # Use in-place operation to set intersection elements to 0\n",
        "    intersection = B[:, column_equality.any(dim=0)]\n",
        "    intersection[:] = 0\n",
        "\n",
        "    # Create a new tensor without intersection elements\n",
        "    B_without_intersection = B[:, ~column_equality.any(dim=0)].clone()\n",
        "\n",
        "    return B_without_intersection\n",
        "\n",
        "  # Display the intersection and B without intersection\n",
        "  #print(\"Intersection:\")\n",
        "  #print(intersection.cpu())\n",
        "  #print(\"B without intersection:\")\n",
        "  #print(B_without_intersection.cpu())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AbVD1oCxrntH"
      },
      "source": [
        "### TuneUP: Synthesizing tail nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "co2V8MIBrntI",
        "outputId": "6ba83136-3b5a-4b19-bc4f-cb8c32c7aa62"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n## USAGE:\\n\\n# percent: rate of edges to keep\\ndata_kept, data_dropped = random_edge_sampler(data, percent)\\n\\n## INPUT:\\nData(x=[169343, 128], edge_index=[2, 1335586], y=[169343, 1])\\n\\n## RETURNS:\\n(Data(edge_index=[2, 468243]), Data(edge_index=[2, 962188]))\\n\\n'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch_geometric.utils import degree\n",
        "\n",
        "def renormalize(edge_index, num_nodes):\n",
        "    # Convert to PyTorch tensor for calculation\n",
        "    edge_index = edge_index.clone().detach()\n",
        "\n",
        "    # Calculate degree and create Degree Matrix D\n",
        "    row, col = edge_index\n",
        "    deg = degree(row, num_nodes, dtype=edge_index.dtype)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
        "\n",
        "    # Renormalize\n",
        "    row, col = edge_index\n",
        "    edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "    return edge_index, edge_weight\n",
        "\n",
        "\n",
        "def random_edge_sampler(data, percent):\n",
        "    edge_index = data.edge_index\n",
        "    num_nodes = data.num_nodes\n",
        "\n",
        "    num_edges = edge_index.size(1)\n",
        "    perm = torch.randperm(num_edges)\n",
        "    preserve_nnz = int(num_edges * percent)\n",
        "\n",
        "    # Indices for kept edges\n",
        "    kept_indices = perm[:preserve_nnz]\n",
        "    kept_edges = edge_index[:, kept_indices]\n",
        "    kept_edges, kept_weights = renormalize(kept_edges, num_nodes)\n",
        "    data_kept = Data(edge_index=kept_edges, edge_attr=kept_weights)\n",
        "\n",
        "    # Indices for dropped edges\n",
        "    dropped_indices = perm[preserve_nnz:]\n",
        "    dropped_edges = edge_index[:, dropped_indices]\n",
        "    dropped_edges, dropped_weights = renormalize(dropped_edges, num_nodes)\n",
        "    data_dropped = Data(edge_index=dropped_edges, edge_attr=dropped_weights)\n",
        "\n",
        "    return data_kept, data_dropped\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## USAGE:\n",
        "\n",
        "# percent: rate of edges to keep\n",
        "data_kept, data_dropped = random_edge_sampler(data, percent)\n",
        "\n",
        "## INPUT:\n",
        "Data(x=[169343, 128], edge_index=[2, 1335586], y=[169343, 1])\n",
        "\n",
        "## RETURNS:\n",
        "(Data(edge_index=[2, 468243]), Data(edge_index=[2, 962188]))\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x4bOCkDTrntI"
      },
      "source": [
        "# Execution\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qfv1uqEWsyrX"
      },
      "source": [
        "### To Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "RrBCxZQ2syXU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "i34DgA0KrntI",
        "outputId": "386e6828-969b-45c3-f52d-1475009a416a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "train loss:  447213.375\n",
            "epoch  1\n",
            "train loss:  418212.6875\n",
            "epoch  2\n",
            "train loss:  395779.40625\n",
            "epoch  3\n",
            "train loss:  378047.90625\n",
            "epoch  4\n",
            "train loss:  363381.71875\n",
            "Validation Loss: 325586.09375\n",
            "time\n",
            "-14.305864334106445\n",
            "recall@50:  tensor(0.1429, device='cuda:0')\n",
            "epoch  5\n",
            "train loss:  350630.0625\n",
            "epoch  6\n",
            "train loss:  340001.90625\n",
            "epoch  7\n",
            "train loss:  330458.125\n",
            "epoch  8\n",
            "train loss:  321583.375\n",
            "epoch  9\n",
            "train loss:  313398.09375\n",
            "Validation Loss: 285687.8125\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-ee380b8dc431>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# L2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_edges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mE_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_edges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_edges_in_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-0471545a282b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, V, data, train_edges, val_edges, optimizer, patience, epochs, test_active)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Validation Loss: {val_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_active\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m           \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_edges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall@50: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Check if early stopping conditions are met\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-94-6d1f5ec174eb>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, nodes, val_edges, z, k)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpositive_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Indicate which edges are positive (1 for positive, 0 for negative)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mpositive_edge_indicator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpositive_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnegative_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GCN(128)\n",
        "\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "E_train = E_train.to(device)\n",
        "E_val = E_val.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)  # L2 regularization\n",
        "\n",
        "train(model, V, data, train_edges=E_train, val_edges=valid_edges_in_V, optimizer=optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH2aqtDjrntI"
      },
      "outputs": [],
      "source": [
        "start_node_dict_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC1XH2FwrntI"
      },
      "outputs": [],
      "source": [
        "v_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "v_mask.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5RRyrpJrntI",
        "outputId": "9631623e-4099-4a13-fd9d-e016420e0462"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([160875])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADFY2PxlrntI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "# this ensures that the current MacOS version is at least 12.3+\n",
        "print(torch.backends.mps.is_available())\n",
        "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
        "print(torch.backends.mps.is_built())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ccesDB8mrntI"
      },
      "source": [
        "data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
